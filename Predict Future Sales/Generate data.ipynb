{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "items_df = pd.read_csv('input/items.csv')\n",
    "shops_df = pd.read_csv('input/shops_yakutsk.csv')\n",
    "icats_df = pd.read_csv('input/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcast types to save memory\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float16)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int8)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating of a new train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "train = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    train.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "train = pd.DataFrame(np.vstack(train), columns = index_cols,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#присоединяем цену\n",
    "item_cnt_month = sales.groupby(['date_block_num', 'shop_id', 'item_id'], as_index = False)['item_cnt_day'].sum()\n",
    "train = pd.merge(train, item_cnt_month, on = ['date_block_num', 'shop_id', 'item_id'], how = 'left')\n",
    "train.rename(columns = {'item_cnt_day' : 'item_cnt_month'}, inplace = True)\n",
    "\n",
    "train['item_cnt_month'] = train['item_cnt_month'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 34th month\n",
    "test[\"date_block_num\"] = train['date_block_num'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add info from addition files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing of additional tables\n",
    "def take_last(x):\n",
    "    s = x.split('-')\n",
    "    if len(s) > 1:\n",
    "        return s[-1]\n",
    "    return ''\n",
    "\n",
    "#items\n",
    "subcategory = icats_df['item_category_name'].apply(take_last)\n",
    "unique_splt = subcategory.unique()\n",
    "submapped = dict(zip(unique_splt , range(len(unique_splt)))) \n",
    "icats_df['subcategory'] = subcategory.map(submapped)\n",
    "\n",
    "supercategory = icats_df['item_category_name'].apply(lambda x: x.split('-')[0])\n",
    "unique_splt = supercategory.unique()\n",
    "supermapped = dict(zip(unique_splt , range(len(unique_splt)))) \n",
    "icats_df['supercategory'] = supercategory.map(supermapped)\n",
    "\n",
    "#shop\n",
    "city = shops_df['shop_name'].apply(lambda x: x.split(' ')[0])\n",
    "unique_splt = city.unique()\n",
    "city_map = dict(zip(unique_splt , range(len(unique_splt)))) \n",
    "shops_df['city'] = city.map(city_map)\n",
    "\n",
    "shop_type = shops_df['shop_name'].apply(lambda x: x.split(' ')[1])\n",
    "unique_splt = shop_type.unique()\n",
    "shop_type_map = dict(zip(unique_splt , range(len(unique_splt)))) \n",
    "shops_df['shop_type'] = shop_type.map(shop_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "items_merged_cats = pd.merge(items_df, icats_df, on = 'item_category_id', how = 'left')\n",
    "\n",
    "train = pd.merge(train, items_merged_cats, on = 'item_id', how = 'left')\n",
    "test = pd.merge(test, items_merged_cats, on = 'item_id', how = 'left')\n",
    "train = pd.merge(train, shops_df, on = 'shop_id', how = 'left')\n",
    "test = pd.merge(test, shops_df, on = 'shop_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop extra\n",
    "#item_name пока удалять не стоит, тк в них можно поискать паттерны. по типу: 'аудиокниги', 'образовательная литература'\n",
    "train.drop(labels = ['item_category_name', 'shop_name', 'item_name'], axis = 1, inplace = True)\n",
    "test.drop(labels = ['item_category_name', 'shop_name', 'ID', 'item_name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4658: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "summary_shop_sales = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'summary_shop_sales':'sum'}})\n",
    "summary_shop_sales.columns = [col[0] if col[-1]=='' else col[-1] for col in summary_shop_sales.columns.values]\n",
    "\n",
    "summary_item_sales = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'summary_item_sales':'sum'}})\n",
    "summary_item_sales.columns = [col[0] if col[-1] == '' else col[-1] for col in summary_item_sales.columns.values]\n",
    "\n",
    "summary_sales = sales.groupby(['date_block_num'],as_index=False).agg({'item_cnt_day':{'summary_sales':'sum'}})\n",
    "summary_sales.columns = [col[0] if col[-1] == '' else col[-1] for col in summary_sales.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4658: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "mean_sales = sales.groupby(['date_block_num'],as_index=False).agg({'item_cnt_day':{'mean_sales':'mean'}})\n",
    "mean_sales.columns = [col[0] if col[-1] == '' else col[-1] for col in mean_sales.columns.values]\n",
    "\n",
    "mean_shop_sales = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'mean_shop_sales':'mean'}})\n",
    "mean_shop_sales.columns = [col[0] if col[-1]=='' else col[-1] for col in mean_shop_sales.columns.values]\n",
    "\n",
    "mean_item_sales = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'mean_item_sales':'mean'}})\n",
    "mean_item_sales.columns = [col[0] if col[-1] == '' else col[-1] for col in mean_item_sales.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with the train\n",
    "train = pd.merge(train, summary_shop_sales, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "train = pd.merge(train, summary_item_sales, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "train = pd.merge(train, summary_sales, how='left', on=['date_block_num']).fillna(0)\n",
    "train = pd.merge(train, mean_shop_sales, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "train = pd.merge(train, mean_item_sales, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "train = pd.merge(train, mean_sales, how='left', on=['date_block_num']).fillna(0)\n",
    "\n",
    "\n",
    "test = pd.merge(test, summary_shop_sales, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "test = pd.merge(test, summary_item_sales, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "test = pd.merge(test, summary_sales, how='left', on=['date_block_num']).fillna(0)\n",
    "test = pd.merge(test, mean_shop_sales, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "test = pd.merge(test, mean_item_sales, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "test = pd.merge(test, mean_sales, how='left', on=['date_block_num']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test, sales, items_df, shops_df, icats_df, item_cnt_month, items_merged_cats, summary_shop_sales,\n",
    "del summary_item_sales, summary_sales, mean_shop_sales, mean_item_sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns that we will be used to create shift\n",
    "cols_to_rename = ['item_cnt_month', 'summary_item_sales', 'summary_shop_sales']\n",
    "\n",
    "for month_shift in [1, 2, 3, 4]:\n",
    "    data_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    #взять 10 месяц из олдаты, сдвинуть его вперед(например до 11-го) и смерджить 11 получившийся месяц к 10-у\n",
    "    data_shift['date_block_num'] = data_shift['date_block_num'] + month_shift\n",
    "    #change name\n",
    "    foo = lambda x: '{}_shift_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    data_shift = data_shift.rename(columns = foo)\n",
    "    all_data = pd.merge(all_data, data_shift, on = index_cols, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[all_data['date_block_num'] != (all_data['date_block_num'].max())]\n",
    "test = all_data[all_data['date_block_num'] == (all_data['date_block_num'].max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I shouldn't use old data from year 2013\n",
    "train = train[(train[\"date_block_num\"] > 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train.drop(labels = ['summary_item_sales', 'summary_shop_sales'], axis = 1, inplace = True)\n",
    "test.drop(labels = ['summary_item_sales', 'summary_shop_sales', 'item_cnt_month'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('generated/train.csv', index=False)\n",
    "test.to_csv('generated/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
